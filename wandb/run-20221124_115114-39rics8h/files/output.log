Dataset: Nyuv2 | Training Task: Semantic | Primary Task: Semantic in Multi-task / Auxiliary Learning Mode with SEGMENTATION
Applying Multi-task Methods: Weighting-based: Equal + Gradient-based: NONE
STEP. Loading datasets...
Data sanity check. RGB.shape: torch.Size([16, 3, 288, 384]),	Depth.shape torch.Size([16, 1, 288, 384]),    	Semantic.shape torch.Size([16, 288, 384]),	Normals.shape torch.Size([16, 3, 288, 384])
  0%|                                                                                                                                                           | 0/50 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "train_model.py", line 277, in <module>
    train_loss = [compute_loss(train_pred[i], train_target[task_id], task_id) for i, task_id in enumerate(train_tasks)]
  File "train_model.py", line 277, in <listcomp>
    train_loss = [compute_loss(train_pred[i], train_target[task_id], task_id) for i, task_id in enumerate(train_tasks)]
  File "/home/dim/mdpi_robotics/mtl-world/utils.py", line 110, in compute_loss
    loss = F.cross_entropy(pred, gt, ignore_index=-1)
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/functional.py", line 2846, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
ValueError: Expected input batch_size (23) to match target batch_size (16).