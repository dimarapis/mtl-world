DDRNetMTL_Split
Dataset: Sim_Warehouse | Training Task: Depth + Semantic + Normals | Primary Task: Depth + Semantic + Normals in Multi-task / Auxiliary Learning Mode with DDRNET
Applying Multi-task Methods: Weighting-based: Equal + Gradient-based: None
STEP. Loading datasets...
Data sanity check. RGB.shape: torch.Size([32, 3, 360, 640]),	Depth.shape torch.Size([32, 1, 360, 640]),        	Semantic.shape torch.Size([32, 1, 720, 1280]),	Normals.shape torch.Size([32, 3, 360, 640])
  0%|                                                                                                            | 0/60 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/robotlabx/mtl-world/train_model.py", line 630, in <module>
    training(opt)
  File "/home/robotlabx/mtl-world/train_model.py", line 335, in training
    train_loss = [compute_loss_ole(train_pred[i], train_target[task_id], task_id) for i, task_id in enumerate(train_tasks)]
  File "/home/robotlabx/mtl-world/train_model.py", line 335, in <listcomp>
    train_loss = [compute_loss_ole(train_pred[i], train_target[task_id], task_id) for i, task_id in enumerate(train_tasks)]
  File "/home/robotlabx/mtl-world/utils.py", line 170, in compute_loss_ole
    loss = F.cross_entropy(pred, gt, ignore_index=-1)
  File "/home/robotlabx/anaconda3/envs/multisim/lib/python3.9/site-packages/torch/nn/functional.py", line 3026, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected target size [32, 640], got [32, 720, 1280]